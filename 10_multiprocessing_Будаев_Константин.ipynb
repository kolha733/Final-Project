{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad7d57b",
   "metadata": {
    "id": "bad7d57b"
   },
   "source": [
    "# Лабораторная Работа №10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70485c2d-3eb7-4dbe-b72e-8a0f564bb972",
   "metadata": {
    "id": "70485c2d-3eb7-4dbe-b72e-8a0f564bb972"
   },
   "source": [
    "## Параметры и импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78da5ea3",
   "metadata": {
    "id": "78da5ea3"
   },
   "outputs": [],
   "source": [
    "# === ПАРАМЕТРЫ (можно менять под свои условия) ===============================\n",
    "from pathlib import Path\n",
    "\n",
    "# Путь к исходному датасету. Можно указать .xlsx или .csv\n",
    "DATA_PATH = Path('recipes_full.csv')\n",
    "\n",
    "# Директория, куда будут сохраняться части\n",
    "OUT_DIR = Path('results/')\n",
    "\n",
    "# Сколько частей делаем (примерно одинаковых по объёму)\n",
    "N_PARTS = 8\n",
    "\n",
    "# Разделитель в выходных CSV (по заданию — точка с запятой)\n",
    "DELIM = ';'\n",
    "\n",
    "# Имя шаблона выходных файлов\n",
    "PART_PREFIX = 'id_tag_nsteps_'\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4a4d3f",
   "metadata": {
    "id": "3d4a4d3f"
   },
   "outputs": [],
   "source": [
    "# Импорты. openpyxl нужен только для .xlsx\n",
    "import csv, ast, time, os\n",
    "from typing import Dict, Tuple, Iterable, List\n",
    "from multiprocessing import Process, Queue, cpu_count\n",
    "\n",
    "try:\n",
    "    from openpyxl import load_workbook  # Для потокового чтения XLSX\n",
    "except Exception:\n",
    "    load_workbook = None  # Если пакет не установлен, остаётся поддержка только CSV\n",
    "\n",
    "# Создаём выходную директорию (если её ещё нет)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa65fec",
   "metadata": {
    "id": "dfa65fec"
   },
   "source": [
    "## Универсальный потоковый итератор для `.xlsx` и `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9bb347",
   "metadata": {
    "id": "cb9bb347"
   },
   "outputs": [],
   "source": [
    "def iter_recipes_unified(path: Path):\n",
    "    \"\"\"Итератор, который **построчно** (стримингово) возвращает словари вида:\n",
    "        {'id': int, 'n_steps': int, 'tags': list[str]}\n",
    "\n",
    "    Поддерживает два формата источника:\n",
    "      • XLSX: каждая ячейка первой колонки содержит целую CSV-строку набора рецептов;\n",
    "               мы читаем лист через openpyxl в режиме read_only и парсим каждую строку как CSV.\n",
    "      • CSV : обычный CSV-файл с заголовком; читаем заголовок, находим индексы нужных колонок\n",
    "               и далее читаем строки без загрузки всего файла в память.\n",
    "\n",
    "    Такой подход не требует больших ресурсов и позволяет обрабатывать очень крупные файлы.\n",
    "    \"\"\"\n",
    "    suf = path.suffix.lower()\n",
    "\n",
    "    # --------------------------------- XLSX ----------------------------------\n",
    "    if suf in ('.xlsx', '.xls'):\n",
    "        if load_workbook is None:\n",
    "            raise RuntimeError('openpyxl недоступен: нельзя прочитать XLSX')\n",
    "\n",
    "        # Открываем книгу в режиме \"только чтение\" (эффективно по памяти)\n",
    "        wb = load_workbook(filename=path, read_only=True, data_only=True)\n",
    "        ws = wb.active  # используем активный лист\n",
    "\n",
    "        first = True  # флаг для пропуска заголовка (первая CSV-строка в файле)\n",
    "        try:\n",
    "            # Итерация по строкам листа отдаёт кортежи со значениями ячеек\n",
    "            for row in ws.iter_rows(values_only=True):\n",
    "                if not row:\n",
    "                    continue  # пустая строка\n",
    "                line = row[0]  # в наборе — всё содержится в первой колонке\n",
    "                if line is None:\n",
    "                    continue\n",
    "\n",
    "                # В ячейке лежит полноценная CSV-строка — парсим её корректно (учёт кавычек).\n",
    "                for cols in csv.reader([line]):\n",
    "                    if first:\n",
    "                        first = False  # это заголовок; пропускаем и двигаемся дальше\n",
    "                        continue\n",
    "                    # Индексы полей соответствуют структуре набора:\n",
    "                    # name(0), id(1), minutes(2), contributor_id(3), submitted(4),\n",
    "                    # tags(5), n_steps(6), steps(7), description(8), ingredients(9), n_ingredients(10)\n",
    "                    try:\n",
    "                        rid = int(cols[1])         # id\n",
    "                        n_steps = int(cols[6])     # количество шагов\n",
    "                    except Exception:\n",
    "                        # Если парсинг не удался — пропускаем строку\n",
    "                        continue\n",
    "                    # Разбираем список тегов из строкового представления Python-списка\n",
    "                    try:\n",
    "                        tags = ast.literal_eval(cols[5])\n",
    "                        if not isinstance(tags, list):\n",
    "                            tags = []\n",
    "                    except Exception:\n",
    "                        tags = []\n",
    "\n",
    "                    # Возвращаем унифицированный словарь\n",
    "                    yield {'id': rid, 'n_steps': n_steps, 'tags': tags}\n",
    "        finally:\n",
    "            wb.close()  # обязательно закрываем книгу\n",
    "\n",
    "    # ---------------------------------- CSV ----------------------------------\n",
    "    elif suf == '.csv':\n",
    "        # Для CSV используем csv.reader (не DictReader), чтобы точно работать с заголовком\n",
    "        # и позициями колонок — это надёжнее, если в данных встречаются необычные кавычки/разделители.\n",
    "        def open_csv(p: Path):\n",
    "            # Пытаемся учесть BOM (utf-8-sig); если его нет — читаем обычным utf-8\n",
    "            try:\n",
    "                return open(p, 'r', encoding='utf-8-sig', newline='')\n",
    "            except Exception:\n",
    "                return open(p, 'r', encoding='utf-8', newline='')\n",
    "\n",
    "        with open_csv(path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader, None)  # читаем строку заголовка\n",
    "            if not header:\n",
    "                return  # пустой файл\n",
    "\n",
    "            # Построим карту \"имя колонки -> индекс\", нормализовав регистр\n",
    "            idx = {name.lower(): i for i, name in enumerate(header)}\n",
    "            i_id    = idx.get('id')\n",
    "            i_steps = idx.get('n_steps')\n",
    "            i_tags  = idx.get('tags')\n",
    "            if i_id is None or i_steps is None or i_tags is None:\n",
    "                raise ValueError('В CSV нет необходимых колонок: id, n_steps, tags')\n",
    "\n",
    "            for cols in reader:\n",
    "                # Пропускаем строки, где не хватает колонок\n",
    "                if len(cols) <= max(i_id, i_steps, i_tags):\n",
    "                    continue\n",
    "                # Аккуратно приводим типы; иногда n_steps может прийти как '11.0'\n",
    "                try:\n",
    "                    rid = int(cols[i_id])\n",
    "                    n_steps = int(float(cols[i_steps]))\n",
    "                except Exception:\n",
    "                    continue\n",
    "                # Разбираем список тегов (формат: \"['tag1','tag2', ...]\")\n",
    "                try:\n",
    "                    tags = ast.literal_eval(cols[i_tags])\n",
    "                    if not isinstance(tags, list):\n",
    "                        tags = []\n",
    "                except Exception:\n",
    "                    tags = []\n",
    "\n",
    "                yield {'id': rid, 'n_steps': n_steps, 'tags': tags}\n",
    "\n",
    "    else:\n",
    "        # Если передали что-то отличное от .xlsx/.csv — сообщаем пользователю\n",
    "        raise ValueError(f'Неподдерживаемое расширение: {suf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d271b",
   "metadata": {
    "id": "7a6d271b"
   },
   "source": [
    "## Задание 1 — разбиение на части `id_tag_nsteps_*.csv`\n",
    "Разбейте файл `recipes_full.csv` на несколько (например, 8) примерно одинаковых по объему файлов c названиями `id_tag_nsteps_*.csv`. Каждый файл содержит 3 столбца: `id`, `tag` и `n_steps`, разделенных символом `;`. Для разбора строк используйте `csv.reader`.\n",
    "\n",
    "__Важно__: вы не можете загружать в память весь файл сразу. Посмотреть на первые несколько строк файла вы можете, написав код, который считывает эти строки.\n",
    "\n",
    "Подсказка: примерное кол-во строк в файле - 2.3 млн.\n",
    "\n",
    "```\n",
    "id;tag;n_steps\n",
    "137739;60-minutes-or-less;11\n",
    "137739;time-to-make;11\n",
    "137739;course;11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba15746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ba15746",
    "outputId": "961ad013-a23f-470e-c70b-5173969eba6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Созданы части: ['id_tag_nsteps_1.csv', 'id_tag_nsteps_2.csv', 'id_tag_nsteps_3.csv', 'id_tag_nsteps_4.csv', 'id_tag_nsteps_5.csv', 'id_tag_nsteps_6.csv', 'id_tag_nsteps_7.csv', 'id_tag_nsteps_8.csv']\n",
      "Размеры (байт): [38846621, 38860091, 38844895, 38853250, 38849311, 38854677, 38848220, 38845459]\n"
     ]
    }
   ],
   "source": [
    "def split_to_parts_id_tag_nsteps_unified(src_path: Path, out_dir: Path, n_parts: int = 8) -> List[Path]:\n",
    "    \"\"\"Читает источники (.xlsx или .csv) **потоково** и создаёт `n_parts` файлов\n",
    "    с колонками `id;tag;n_steps`. Заполнение частей идёт по схеме **round-robin** —\n",
    "    по очереди в каждый файл, чтобы они получались примерно одинакового размера,\n",
    "    даже если заранее неизвестно общее число строк.\n",
    "    \"\"\"\n",
    "    # Готовим выходные файлы и csv.writer'ы\n",
    "    out_paths = [out_dir / f\"{PART_PREFIX}{i+1}.csv\" for i in range(n_parts)]\n",
    "    writers, files = [], []\n",
    "    try:\n",
    "        for p in out_paths:\n",
    "            f = open(p, 'w', newline='', encoding='utf-8')\n",
    "            files.append(f)\n",
    "            w = csv.writer(f, delimiter=DELIM)\n",
    "            w.writerow(['id','tag','n_steps'])  # пишем заголовок\n",
    "            writers.append(w)\n",
    "\n",
    "        # Индекс текущего файла для round-robin (0..n_parts-1)\n",
    "        rr = 0\n",
    "\n",
    "        # Идём по рецептам и раскладываем пары (id, tag, n_steps)\n",
    "        for rec in iter_recipes_unified(src_path):\n",
    "            rid = rec['id']\n",
    "            n_steps = rec['n_steps']\n",
    "            tags = rec['tags'] or []\n",
    "            if rid is None or n_steps is None:\n",
    "                continue  # перестраховка: пропустить странную строку\n",
    "\n",
    "            for tag in tags:\n",
    "                if not tag:\n",
    "                    continue\n",
    "                t = str(tag).strip()\n",
    "                if not t:\n",
    "                    continue\n",
    "                # Записываем строку и переходим к следующему «ведру»\n",
    "                writers[rr].writerow([rid, t, n_steps])\n",
    "                rr = (rr + 1) % n_parts\n",
    "    finally:\n",
    "        # Гарантированно закрываем все файлы (даже при исключениях)\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return out_paths\n",
    "\n",
    "# Выполним разбиение прямо сейчас\n",
    "parts = split_to_parts_id_tag_nsteps_unified(DATA_PATH, OUT_DIR, N_PARTS)\n",
    "print('Созданы части:', [p.name for p in parts])\n",
    "print('Размеры (байт):', [p.stat().st_size for p in parts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344da50",
   "metadata": {
    "id": "5344da50"
   },
   "source": [
    "## Задание 2 — среднее `n_steps` по тегам для **одного** файла\n",
    "Напишите функцию, которая принимает на вход название файла, созданного в результате решения задачи 1, считает среднее значение количества шагов для каждого тэга и возвращает результат в виде словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692f68e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "692f68e6",
    "outputId": "df056ac8-74d2-4840-a5b7-542dfbf5cf43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример 10 тегов из id_tag_nsteps_1.csv :\n",
      "mexican -> 5.32\n",
      "ham-and-bean-soup -> 3.53\n",
      "quick-breads -> 5.02\n",
      "60-minutes-or-less -> 9.50\n",
      "dinner-party -> 8.22\n",
      "course -> 9.25\n",
      "chicken -> 7.26\n",
      "veal -> 3.61\n",
      "dips-summer -> 3.56\n",
      "bacon -> 4.00\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def tag_stats_from_part(csv_path: Path) -> Dict[str, Tuple[int,int]]:\n",
    "    \"\"\"Считает промежуточные статистики по одному файлу-части:\n",
    "       возвращает словарь `tag -> (sum_steps, count)`.\n",
    "       Такой формат удобен для последующего корректного объединения результатов.\n",
    "    \"\"\"\n",
    "    stats: Dict[str, Tuple[int,int]] = {}\n",
    "    with open(csv_path, 'r', encoding='utf-8', newline='') as f:\n",
    "        r = csv.reader(f, delimiter=DELIM)\n",
    "        next(r, None)  # пропускаем заголовок\n",
    "        for row in r:\n",
    "            if len(row) != 3:\n",
    "                continue\n",
    "            _rid, tag, n_steps = row\n",
    "            if not tag:\n",
    "                continue\n",
    "            try:\n",
    "                n = int(n_steps)\n",
    "            except Exception:\n",
    "                continue\n",
    "            # Накапливаем сумму шагов и количество вхождений для тега\n",
    "            s,c = stats.get(tag, (0,0))\n",
    "            stats[tag] = (s+n, c+1)\n",
    "    return stats\n",
    "\n",
    "def tag_means_from_part(csv_path: Path) -> Dict[str, float]:\n",
    "    \"\"\"Возвращает `tag -> среднее n_steps` для одного файла.\"\"\"\n",
    "    stats = tag_stats_from_part(csv_path)\n",
    "    return {t: (s / c) for t, (s, c) in stats.items() if c > 0}\n",
    "\n",
    "# Мини-проверка на первой части\n",
    "example = tag_means_from_part(parts[0])\n",
    "print('Пример 10 тегов из', parts[0].name, ':')\n",
    "for k in list(example.keys())[:10]:\n",
    "    print(k, f\"-> {example[k]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774f4de",
   "metadata": {
    "id": "7774f4de"
   },
   "source": [
    "## Задание 3 — объединение результатов по всем файлам (последовательно)\n",
    "Напишите функцию, которая считает среднее значение количества шагов для каждого тэга по всем файлам, полученным в задаче 1, и возвращает результат в виде словаря. Не используйте параллельных вычислений. При реализации выделите функцию, которая объединяет результаты обработки отдельных файлов. Модифицируйте код из задачи 2 таким образом, чтобы иметь возможность получить результат, имея результаты обработки отдельных файлов. Определите, за какое время задача решается для всех файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1919cb8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1919cb8e",
    "outputId": "58a5d09e-4894-49f3-fdfc-1fb0937624b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Последовательно: 8 файлов за 10.6719 с; тегов: 551\n",
      "Примеры 10 тегов:\n",
      "mexican -> 5.30\n",
      "ham-and-bean-soup -> 3.51\n",
      "quick-breads -> 5.06\n",
      "60-minutes-or-less -> 9.41\n",
      "dinner-party -> 8.23\n",
      "course -> 9.27\n",
      "chicken -> 7.33\n",
      "veal -> 3.68\n",
      "dips-summer -> 3.48\n",
      "bacon -> 4.11\n"
     ]
    }
   ],
   "source": [
    "def merge_tag_stats(dicts: Iterable[Dict[str, Tuple[int,int]]]) -> Dict[str, Tuple[int,int]]:\n",
    "    \"\"\"Складывает несколько `tag -> (sum, count)` в один словарь.\"\"\"\n",
    "    merged: Dict[str, Tuple[int,int]] = {}\n",
    "    for d in dicts:\n",
    "        for tag, (s, c) in d.items():\n",
    "            S, C = merged.get(tag, (0, 0))\n",
    "            merged[tag] = (S + s, C + c)\n",
    "    return merged\n",
    "\n",
    "def sequential_all_means(files: List[Path]):\n",
    "    \"\"\"Последовательно обрабатывает все части и считает финальные средние по тегам.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    parts_stats = [tag_stats_from_part(p) for p in files]\n",
    "    merged = merge_tag_stats(parts_stats)\n",
    "    means = {t: (s / c) for t, (s, c) in merged.items() if c > 0}\n",
    "    dt = time.perf_counter() - t0\n",
    "    return means, dt\n",
    "\n",
    "means_seq, t_seq = sequential_all_means(parts)\n",
    "print(f'Последовательно: {len(parts)} файлов за {t_seq:.4f} с; тегов: {len(means_seq)}')\n",
    "print('Примеры 10 тегов:')\n",
    "for k in list(means_seq.keys())[:10]:\n",
    "    print(k, f\"-> {means_seq[k]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1aa64b",
   "metadata": {
    "id": "dd1aa64b"
   },
   "source": [
    "## Задание 4 — `multiprocessing`: отдельный процесс на каждый файл\n",
    "Решите задачу 3, распараллелив вычисления с помощью модуля `multiprocessing`. Для обработки каждого файла создайте свой собственный процесс. Определите, за какое время задача решается для всех файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e24b7d5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e24b7d5c",
    "outputId": "4b147dfa-269a-4321-cd2c-819275fc8437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp(one-per-file): 8 файлов за 11.5837 с; тегов: 551\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue, cpu_count\n",
    "\n",
    "def worker_file_stats(path: str, out_q: Queue):\n",
    "    \"\"\"Рабочий процесс: считает `tag_stats_from_part` и складывает результат в очередь.\"\"\"\n",
    "    out_q.put(tag_stats_from_part(Path(path)))\n",
    "\n",
    "def mp_one_proc_per_file(files: List[Path]):\n",
    "    \"\"\"Запускает по **отдельному процессу на каждый файл**.\n",
    "       Коммуникация через очередь: каждый процесс кладёт в неё словарь статистик.\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    q = Queue()\n",
    "    procs = []\n",
    "    for p in files:\n",
    "        pr = Process(target=worker_file_stats, args=(str(p), q))\n",
    "        pr.start()\n",
    "        procs.append(pr)\n",
    "\n",
    "    # Забираем результаты от всех процессов\n",
    "    collected = [q.get() for _ in files]\n",
    "\n",
    "    # Дожидаемся завершения\n",
    "    for pr in procs:\n",
    "        pr.join()\n",
    "\n",
    "    merged = merge_tag_stats(collected)\n",
    "    means = {t: (s / c) for t, (s, c) in merged.items() if c > 0}\n",
    "    dt = time.perf_counter() - t0\n",
    "    return means, dt\n",
    "\n",
    "means_mp1, t_mp1 = mp_one_proc_per_file(parts)\n",
    "print(f'mp(one-per-file): {len(parts)} файлов за {t_mp1:.4f} с; тегов: {len(means_mp1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d519ca9",
   "metadata": {
    "id": "3d519ca9"
   },
   "source": [
    "## Задание 5 — `multiprocessing` с фиксированным числом процессов и очередями\n",
    "(*) Решите задачу 3, распараллелив вычисления с помощью модуля `multiprocessing`. Создайте фиксированное количество процессов (равное половине количества ядер на компьютере). При помощи очереди передайте названия файлов для обработки процессам и при помощи другой очереди заберите от них ответы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442b5355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "442b5355",
    "outputId": "666e1729-b201-4f7d-8926-1e4e80f19c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp(fixed-workers): 8 файлов за 9.8272 с; тегов: 551\n",
      "Совпадение ключей: True\n"
     ]
    }
   ],
   "source": [
    "def worker_fixed(in_q: Queue, out_q: Queue):\n",
    "    \"\"\"Рабочий процесс: берёт путь к файлу из входной очереди, считает статистику\n",
    "       и кладёт результат в выходную очередь. Завершается при получении `None`.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        path = in_q.get()\n",
    "        if path is None:\n",
    "            break  # «ядовитая пилюля» — сигнал завершиться\n",
    "        out_q.put(tag_stats_from_part(Path(path)))\n",
    "\n",
    "def mp_fixed_workers(files: List[Path], n_workers: int | None = None):\n",
    "    \"\"\"Запускает фиксированное число рабочих процессов (по умолчанию: половина ядер).\n",
    "       Задания подаются через очередь `in_q`, результаты собираются из `out_q`.\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() // 2)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    in_q, out_q = Queue(), Queue()\n",
    "\n",
    "    # Стартуем пул рабочих процессов\n",
    "    procs = [Process(target=worker_fixed, args=(in_q, out_q)) for _ in range(n_workers)]\n",
    "    for pr in procs:\n",
    "        pr.start()\n",
    "\n",
    "    # Подаём задания (пути) в очередь\n",
    "    for p in files:\n",
    "        in_q.put(str(p))\n",
    "\n",
    "    # Отправляем каждому процессу «ядовитую пилюлю» — указание завершиться\n",
    "    for _ in procs:\n",
    "        in_q.put(None)\n",
    "\n",
    "    # Собираем результаты\n",
    "    collected = [out_q.get() for _ in files]\n",
    "\n",
    "    # Дожидаемся завершения всех процессов\n",
    "    for pr in procs:\n",
    "        pr.join()\n",
    "\n",
    "    merged = merge_tag_stats(collected)\n",
    "    means = {t: (s / c) for t, (s, c) in merged.items() if c > 0}\n",
    "    dt = time.perf_counter() - t0\n",
    "    return means, dt\n",
    "\n",
    "means_mp2, t_mp2 = mp_fixed_workers(parts)\n",
    "print(f'mp(fixed-workers): {len(parts)} файлов за {t_mp2:.4f} с; тегов: {len(means_mp2)}')\n",
    "print('Совпадение ключей:', set(means_seq.keys()) == set(means_mp1.keys()) == set(means_mp2.keys()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
